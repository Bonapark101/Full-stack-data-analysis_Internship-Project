{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZrkwrJoHtfh0R0UDeTNDB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bonapark101/Full-stack-data-analysis_Internship-Project/blob/main/Data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary**"
      ],
      "metadata": {
        "id": "2PJqz668A3EP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning and Standardizing data before analyzing historical sales performance of spot coffee lots alongside their attributes such as origin, varietal, process, SCA score, profile, and price.\n",
        "\n",
        "***Objectives***\n",
        "1. Identify which sales factors (timing, negotiated price, order pattern) and lot attributes (origin, varietal, process, crop year, SCA score, profile, bag size, initial price, etc.) are linked to good or bad performance\n",
        "2. Learn from past spot coffee performance for successful selection of future specialty spot coffees\n"
      ],
      "metadata": {
        "id": "hxuLF_KYA78N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Description**\n",
        "\n",
        "This dataset combines coffee offerlists from Attesa Coffee. The final merged file contains information about coffee lots, origins, varietals, processes, sca score, and flavour profile.\n",
        "\n",
        "**Column Overview**\n",
        "lot_id = unique identifier for each coffee lot\n",
        "origin = country of origin\n",
        "Varietal = coffee variety\n",
        "process = processing method\n",
        "sca_score = quality score based on SCA cupping standard\n",
        "flavour = flavour category from SCA standard\n",
        "\n",
        "**Data Summary**\n",
        "Time range: 2023 - 2025\n",
        "Data source: PDF and Excel ZIP files provided by Attesa coffee"
      ],
      "metadata": {
        "id": "rTYJaGHju8CL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "***NOTE**\n",
        "\n",
        "I excluded the pricing data, because it is company's internal data.\n"
      ],
      "metadata": {
        "id": "M-7BnJErxX2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing libraries**"
      ],
      "metadata": {
        "id": "cR42LD8sALwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import zipfile\n"
      ],
      "metadata": {
        "id": "_W5o8dgYAWRE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing PDF and Excel files**"
      ],
      "metadata": {
        "id": "y_PsBKmLA112"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PDF and Excel files**\n"
      ],
      "metadata": {
        "id": "6qz12Mqyuzd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert PDF files into CSV\n",
        "# Unzip files in a folder, convert each file\n",
        "\n",
        "# Setting\n",
        "zip_files = [Path(\"/content/22-23.zip\"), Path(\"/content/pdf_dataset_25.zip\")]\n",
        "pdf_folder = Path(\"pdf_files\")\n",
        "csv_folder = Path(\"all_csvs\")\n",
        "\n",
        "pdf_folder.mkdir(exist_ok=True)\n",
        "csv_folder.mkdir(exist_ok=True)\n",
        "\n",
        "# Unzip PDFs\n",
        "for zip_path_pdf in zip_files:\n",
        "    with zipfile.ZipFile(zip_path_pdf, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(pdf_folder)\n",
        "\n",
        "# Convert PDFs to CSVs\n",
        "for pdf_path in pdf_folder.glob(\"*.pdf\"):\n",
        "    all_tables = []\n",
        "    print(pdf_path.name)\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            tables = page.extract_tables()\n",
        "            for table in tables:\n",
        "                df = pd.DataFrame(table)\n",
        "                all_tables.append(df)\n",
        "\n",
        "    if all_tables:\n",
        "        pdf_df = pd.concat(all_tables, ignore_index=True)\n",
        "        csv_name = pdf_path.stem + \".csv\"\n",
        "        pdf_df.to_csv(csv_folder / csv_name, index=False)\n",
        "        print(csv_name)\n",
        "    else:\n",
        "        print(pdf_path.name)\n",
        "\n"
      ],
      "metadata": {
        "id": "KuhFEbhlHnt2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "zip_path_excel = Path(\"/content/23-25.zip\")\n",
        "excel_folder = Path(\"excel_files\")\n",
        "csv_folder = Path(\"all_csvs\")\n",
        "\n",
        "excel_folder.mkdir(exist_ok=True)\n",
        "csv_folder.mkdir(exist_ok=True)\n",
        "\n",
        "# Unzip Excels\n",
        "with zipfile.ZipFile(zip_path_excel, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(excel_folder)\n",
        "\n",
        "# Convert Excel files to CSVs\n",
        "for excel_path in excel_folder.glob(\"*.xls*\"):\n",
        "    print(excel_path.name)\n",
        "    xls = pd.ExcelFile(excel_path)\n",
        "\n",
        "    for sheet_name in xls.sheet_names:\n",
        "        df = pd.read_excel(excel_path, sheet_name=sheet_name, dtype=str)\n",
        "        csv_name = f\"{excel_path.stem}_{sheet_name}.csv\"\n",
        "        df.to_csv(csv_folder / csv_name, index=False)\n",
        "        print(csv_name)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dmJs2gli4Ck0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data cleaning**\n"
      ],
      "metadata": {
        "id": "EjVXs9P3GzGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning files"
      ],
      "metadata": {
        "id": "oDva0KHT3Ate"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preview before cleaning\n",
        "csv_folder = Path(\"/content/all_csvs\")\n",
        "\n",
        "for file in csv_folder.glob(\"*.csv\"):\n",
        "    print(f\"\\nüìÑ Previewing {file.name}\")\n",
        "    df = pd.read_csv(file, dtype=str)  # read everything as string because data in the files are inconsistent\n",
        "    print(df.head())\n",
        "    print(df.info())\n",
        "    print(df.isna().sum())\n",
        "    print(df.duplicated().sum())\n"
      ],
      "metadata": {
        "id": "H70WQJbx3CGi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop irrelevant columns and rows\n",
        "csv_folder = Path(\"/content/all_csvs\")\n",
        "clean_folder = Path(\"/content/cleaned_csvs\")\n",
        "clean_folder.mkdir(exist_ok=True)\n",
        "\n",
        "# Keywords to detect header row\n",
        "keywords = [\"Type\", \"Origin\"]\n",
        "\n",
        "# Columns you may want to drop\n",
        "columns_to_drop = [\n",
        "    'producer', 'status', 'crop_year', 'quantity', 'nan',\n",
        "    'warehouse', 'crop\\nyear', 'bags\\navailable', 'bag_qty\\n(kg)', 'availability'\n",
        "]\n",
        "\n",
        "skipped_files = []\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_folder.glob(\"*.csv\"):\n",
        "    print(f\"Processing {file.name}\")\n",
        "\n",
        "    #Detect header row\n",
        "    df = pd.read_csv(file, dtype=str, header=None)\n",
        "    header_row_idx = None\n",
        "    for i, row in df.iterrows():\n",
        "        if any(any(keyword.lower() in str(cell).lower() for keyword in keywords)\n",
        "               for cell in row if pd.notna(cell)):\n",
        "            header_row_idx = i\n",
        "            break\n",
        "\n",
        "    if header_row_idx is None:\n",
        "        print(f\"No header found in {file.name}, skipping\")\n",
        "        skipped_files.append(file.name)\n",
        "        continue\n",
        "\n",
        "    # Keep rows from header onward\n",
        "    df = df.iloc[header_row_idx:].dropna(how='all').reset_index(drop=True)\n",
        "\n",
        "    # Set the first row as header\n",
        "    df.columns = df.iloc[0].astype(str).str.strip().str.lower().str.replace(' ', '_')\n",
        "    df = df.iloc[1:].reset_index(drop=True)  # remove header row from data\n",
        "\n",
        "    # Drop irrelevant columns\n",
        "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    # Remove rows containing 'WAREHOUSE'\n",
        "    df = df[~df.apply(lambda row: row.astype(str).str.contains('WAREHOUSE', case=False).any(), axis=1)]\n",
        "\n",
        "    # Remove completely empty rows\n",
        "    df = df.dropna(how='all').reset_index(drop=True)\n",
        "\n",
        "    # Save cleaned CSV\n",
        "    cleaned_path = clean_folder / file.name\n",
        "    df.to_csv(cleaned_path, index=False)\n",
        "    print(f\"Saved cleaned CSV: {cleaned_path.name}\")\n",
        "\n",
        "    # Preview cleaned data\n",
        "    print(\"First 5 rows:\")\n",
        "    print(df.head())\n",
        "    print(df.info())\n",
        "    print(\"Missing values per column:\\n\", df.isna().sum())\n",
        "    print(\"Number of duplicate rows:\", df.duplicated().sum())\n",
        "\n",
        "    # List skipped files\n",
        "if skipped_files:\n",
        "    print(\"\\nSkipped files (no header detected):\")\n",
        "    for f in skipped_files:\n",
        "        print(f\"- {f}\")\n"
      ],
      "metadata": {
        "id": "vh0wu2QA3EM6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_folder = Path(\"/content/cleaned_csvs\")\n",
        "\n",
        "deleted_files = []\n",
        "\n",
        "for csv_file in csv_folder.glob(\"*.csv\"):\n",
        "    if \"2022\" in csv_file.name:\n",
        "        csv_file.unlink()  # delete the file\n",
        "        deleted_files.append(csv_file.name)\n",
        "        print(f\"Deleted file: {csv_file.name}\")"
      ],
      "metadata": {
        "id": "7gpIb2DcKYwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the processed files\n",
        "csv_folder = Path(\"cleaned_csvs\")\n",
        "\n",
        "for csv_file in csv_folder.glob(\"*.csv\"):\n",
        "    print(f\"\\nPreview of {csv_file.name}\")\n",
        "    try:\n",
        "        # Read the cleaned CSV\n",
        "        df = pd.read_csv(csv_file, dtype=str)\n",
        "\n",
        "        # Show first 5 rows\n",
        "        print(df.head(5))\n",
        "\n",
        "        # Column names and basic shape\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "        print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "\n",
        "        # Check missing values and duplicates\n",
        "        print(\"Missing values per column:\\n\", df.isna().sum())\n",
        "        print(\"Number of duplicate rows:\", df.duplicated().sum())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not read {csv_file.name}: {e}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QmATxK_QJ71S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Standardization**"
      ],
      "metadata": {
        "id": "BDX504Q_IKrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sca score and pricing-related columns have not properly transferred;\n",
        "# so had to go through few attempts, but only went through for sca score, as pricing should not be publicly posted\n",
        "csv_folder = Path(\"/content/cleaned_csvs\")\n",
        "standardized_folder = Path(\"/content/standardized_csvs\")\n",
        "standardized_folder.mkdir(exist_ok=True)\n",
        "\n",
        "# Column mapping for other fields\n",
        "column_mapping = {\n",
        "    \"Lot ID\": \"lot_id\",\n",
        "    \"LotID\": \"lot_id\",\n",
        "    \"Origin\": \"origin\",\n",
        "    \"Country of Origin\": \"origin\",\n",
        "    \"Type\": \"type\",\n",
        "    \"Process\": \"process\",\n",
        "    \"Varietal\": \"varietal\",\n",
        "    \"Profile\": \"profile\",\n",
        "    \"600+ kg\": \"over_600kg\",\n",
        "    \"60+ kg\": \"over_60kg\",\n",
        "    \"1.8+ tons\": \"over_1800kg\",\n",
        "    \"3.6+ tons\": \"over_3600kg\",\n",
        "    \"1+ bag\": \"over_60kg\",\n",
        "    \"3+ bags\": \"over_180kg\",\n",
        "    \"10+ bags\": \"over_600kg\",\n",
        "    \"30+ bags\": \"over_1800kg\",\n",
        "    \"30+ Bags\": \"over_1800kg\",\n",
        "    \"60+ bags\": \"over_3600kg\",\n",
        "    \"60+ Bags\": \"over_3600kg\",\n",
        "}\n",
        "\n",
        "# Expected final schema\n",
        "expected_columns = [\n",
        "    \"lot_id\", \"origin\", \"type\", \"process\", \"varietal\",\n",
        "    \"sca_score\", \"profile\", \"over_60kg\", \"over_180kg\",\n",
        "    \"over_600kg\", \"over_1800kg\", \"over_3600kg\"\n",
        "]\n",
        "\n",
        "for csv_file in csv_folder.glob(\"*.csv\"):\n",
        "    df = pd.read_csv(csv_file, dtype=str)\n",
        "\n",
        "    # Normalize column names: strip, lowercase, replace non-alphanumeric chars with underscore\n",
        "    df.columns = [str(c).strip().lower().replace(' ', '_').replace('\\\\', '').replace('/', '') for c in df.columns]\n",
        "\n",
        "    # Detect SCA score column automatically\n",
        "    sca_col_candidates = [c for c in df.columns if 'sca' in c and 'score' in c]\n",
        "    if sca_col_candidates:\n",
        "        df = df.rename(columns={sca_col_candidates[0]: 'sca_score'})\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è No SCA score column found in {csv_file.name}\")\n",
        "\n",
        "    # Map other columns using column_mapping\n",
        "    df = df.rename(columns=lambda x: column_mapping.get(x, x))\n",
        "\n",
        "    # Add missing columns\n",
        "    for col in expected_columns:\n",
        "        if col not in df.columns:\n",
        "            df[col] = pd.NA\n",
        "\n",
        "    # Reorder columns\n",
        "    df = df[expected_columns]\n",
        "\n",
        "    # Save standardized CSV\n",
        "    standardized_path = standardized_folder / csv_file.name\n",
        "    df.to_csv(standardized_path, index=False)\n",
        "    print(f\"Standardized {csv_file.name}\")\n",
        "\n",
        "print(\"‚úÖ All files standardized and SCA scores preserved.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "urOAuHH6QvCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standardized_folder = Path(\"/content/standardized_csvs\")\n",
        "\n",
        "# Collect all standardized CSV files and merge them all\n",
        "all_files = list(standardized_folder.glob(\"*.csv\"))\n",
        "combined = pd.concat(\n",
        "    [pd.read_csv(f, dtype=str) for f in all_files],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "print(f\"Merged {len(all_files)} standardized files ‚Äî total rows: {len(combined)}\")\n",
        "\n",
        "# Optional: check structure\n",
        "print(\"Columns:\", list(combined.columns))\n",
        "print(\"\\nPreview of merged data:\")\n",
        "print(combined.head())\n"
      ],
      "metadata": {
        "id": "vetecoQBVLT1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the merged file\n",
        "output_path = Path(\"/content/final_merged_standardized.csv\")\n",
        "combined.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\nFinal merged dataset saved successfully at: {output_path.resolve()}\")\n"
      ],
      "metadata": {
        "id": "uLuq_kUhlsXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Standardization**"
      ],
      "metadata": {
        "id": "tBqhcOVDHZJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardizing_flavour profile"
      ],
      "metadata": {
        "id": "6hr1ANf22EqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize flavour profile according to SCA standard\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "df = pd.read_csv('/content/final_merged_standardized.csv')\n",
        "\n",
        "# Define SCA inner-ring mapping\n",
        "sca_inner_map = {\n",
        "    'Fruity': ['blackberry', 'raspberry', 'blueberry', 'strawberry', 'raisin',\n",
        "               'prune', 'coconut', 'cherry', 'pomegranate', 'pineapple',\n",
        "               'grape', 'apple', 'peach', 'pear', 'grapefruit', 'orange', 'lemon', 'lime', 'mandarin'],\n",
        "    'Sweet': ['brown sugar', 'vanilla', 'honey', 'caramelized', 'maple syrup', 'molasses', 'caramel'],\n",
        "    'Nutty/Cocoa': ['peanut', 'almond', 'hazelnut', 'chocolate', 'dark chocolate', 'milk chocolate', 'cacao', 'cocoa'],\n",
        "    'Spices': ['cinnamon', 'clove', 'anise', 'nutmeg', 'pepper', 'pungent'],\n",
        "    'Roasted': ['cereal', 'burnt','tobacco', 'pipe tobacco', 'malt', 'grain', 'brown', 'roast', 'smoky', 'ashy', 'acrid'],\n",
        "    'Floral': ['black tea', 'chamomile','jasmine', 'rose'],\n",
        "    'Sour/Fermented': ['sour', 'alcohol', 'fermented', 'citric acid', 'malic acid', 'winey', 'whiskey', 'overripe'],\n",
        "    'Green/Vegetative': ['olive oil', 'raw', 'green', 'vegetable', 'beany', 'under_ripe', 'peapod', 'fresh', 'herbal', 'hay', 'herb_like'],\n",
        "    'Other': ['chemical', 'papery', 'musty', 'stale', 'cardboard', 'woody', 'moldy', 'earthy', 'animalic', 'meaty', 'brothy']\n",
        "}\n",
        "\n",
        "# Flatten valid terms\n",
        "valid_terms = {term.lower() for group in sca_inner_map.values() for term in group}\n",
        "\n",
        "# Clean and validate flavor profiles\n",
        "def keep_valid_terms(profile):\n",
        "    if pd.isna(profile):\n",
        "        return np.nan\n",
        "    # Split by commas, slashes, or semicolons\n",
        "    words = re.split(r'[,/;]', profile.lower())\n",
        "    words = [w.strip() for w in words if w.strip()]\n",
        "    valid = [w for w in words if w in valid_terms]\n",
        "    return ', '.join(sorted(set(valid))) if valid else np.nan\n",
        "\n",
        "# Apply cleaning (keep all rows)\n",
        "if 'profile' in df.columns:\n",
        "    df['Profile_SCA_Valid'] = df['profile'].apply(keep_valid_terms)\n",
        "else:\n",
        "    raise KeyError(\"Column 'Profile' not found in the dataset. Please check your column names.\")\n",
        "\n",
        "# Preview result\n",
        "print(\"Processed dataset ‚Äî first 10 rows:\")\n",
        "print(df[['profile', 'Profile_SCA_Valid']].head(10))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pPil7Peyjzzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the Profile_SCA_Valid values into SCA outer-ring profile\n",
        "term_to_category = {term.lower(): category for category, terms in sca_inner_map.items() for term in terms}\n",
        "\n",
        "# Function to map profile to standard_flavour\n",
        "def map_to_standard_flavour(profile):\n",
        "    if pd.isna(profile):\n",
        "        return np.nan\n",
        "    # Split by comma, slash, semicolon\n",
        "    terms = [w.strip().lower() for w in re.split(r'[,/;]', profile)]\n",
        "    # Map each term to its category\n",
        "    categories = set(term_to_category.get(t) for t in terms if t in term_to_category)\n",
        "    if categories:\n",
        "        return ', '.join(sorted(categories))\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "# Apply to the dataset\n",
        "df['standard_flavour'] = df['Profile_SCA_Valid'].apply(map_to_standard_flavour)\n",
        "\n",
        "# Inspect result\n",
        "df[['Profile_SCA_Valid', 'standard_flavour']].head(20)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fcJ3Tw8Lulgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate rows based on 'lot_id', keep the first occurrence\n",
        "df = df.drop_duplicates(subset=['lot_id'], keep='first').reset_index(drop=True)\n",
        "\n",
        "# Preview result\n",
        "print(f\"After removing duplicates, total rows: {len(df)}\")\n",
        "df['lot_id'].head(10)\n"
      ],
      "metadata": {
        "id": "gBEoq0M9xj5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change data type_sca_score, over_60kg, over_180kg, over_600kg, over_600kg, over_1800kg, over_3600kg\n",
        "# List of columns to convert\n",
        "numeric_columns = [\n",
        "    'sca_score',\n",
        "    'over_60kg',\n",
        "    'over_180kg',\n",
        "    'over_600kg',\n",
        "    'over_1800kg',\n",
        "    'over_3600kg'\n",
        "]\n",
        "\n",
        "# Convert to float\n",
        "for col in numeric_columns:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Check the data types\n",
        "print(df[numeric_columns].dtypes)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DL-1iTc8qlwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only rows where lot_id is a string and starts with 'ATT'\n",
        "mask = df['lot_id'].astype(str).str.strip().str.upper().str.startswith('ATT')\n",
        "final_df = df[mask].reset_index(drop=True)\n",
        "\n",
        "# Preview the first few rows to confirm\n",
        "print(f\"‚úÖ Rows after filtering lot_id starting with 'ATT': {len(final_df)}\")\n",
        "print(final_df[['lot_id']].head())\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "m1xz6RMdAOrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'type', 'over_60kg', 'over_180kg', 'over_600kg', 'over_1800kg', 'over_3600kg' column if it exists\n",
        "columns_to_drop = ['type', 'over_60kg', 'over_180kg', 'over_600kg', 'over_1800kg', 'over_3600kg']\n",
        "\n",
        "# Drop them if they exist\n",
        "existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
        "\n",
        "if existing_columns_to_drop:\n",
        "    df = df.drop(columns=existing_columns_to_drop)\n",
        "    print(f\"Dropped columns: {existing_columns_to_drop}\")\n",
        "else:\n",
        "    print(\"None of the specified columns were found.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uv057PkwCeoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the file for analysis\n",
        "\n",
        "# Define the path where you want to save\n",
        "output_path = Path(\"/content/final_attesa.csv\")\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Final cleaned dataset saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "XgwDHPY5CooV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}